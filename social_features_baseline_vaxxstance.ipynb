{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VaxxStance_Baseline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKlN+CAcmEu857qlYPGZVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaxxstance/vaxxstance.github.io/blob/main/VaxxStance_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcyeZeEZAID3"
      },
      "source": [
        "## VaxxStance@IberLEF 2021 Contextual Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUV_rtwfBw5b"
      },
      "source": [
        "Mount your drive in order to access the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl2Rdha2IiQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b7d347-afcc-4fed-c486-ba52b75df7ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AyjdmaDIYHv"
      },
      "source": [
        "The activities of the user in social networks will be measured with the following function. \n",
        "\n",
        "For this task, we measure twitter activities in terms of user status in twitter with the following measurements: \n",
        "\n",
        "- statuses_count  \n",
        "- friends_count  \n",
        "- followers_count  \n",
        "- created_at  \n",
        "- emoji (emojis in bio)\n",
        "\n",
        "(Data available at file USER.csv)\n",
        "\n",
        "\n",
        "On the other hand, the information related to the tweet itself will be measured with: \n",
        "\n",
        "- retweet_count  \n",
        "- favorite_count  \n",
        "- source  (FB, TWITTER)\n",
        "- created_at\n",
        "\n",
        "(Data available at file TWEETS.csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrwfQqD7IYHw"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def getTwitterActionsFeatures(df_origin):\n",
        "  df_tweets = pd.read_csv('/content/drive/path/to/dataset/tweet.csv')\n",
        "  df_user = pd.read_csv('/content/drive/path/to/dataset/user.csv')\n",
        "  df_merge = pd.merge(df_origin, df_tweets, on = ['user_id', 'tweet_id'])\n",
        "  df_merged = pd.merge(df_merge, df_user, on = ['user_id'])\n",
        "  return df_merged\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDIkcirIIYH1"
      },
      "source": [
        "# We build a train dataset that contains all the features that we want to use for training.\n",
        "\n",
        "df = pd.read_csv('/content/drive/path/to/dataset/train.csv')\n",
        "df_train = getTwitterActionsFeatures(df)\n",
        "df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1CvutPDDWaE"
      },
      "source": [
        "#data transformations:\n",
        "# 1. labels to categorical codes\n",
        "df_train.label = pd.Categorical(df_train.label)\n",
        "df_train['label'] = df_train.label.cat.codes\n",
        "# 2. source labels(twitter, fb) to  categorical codes\n",
        "df_train['source'] = ('twitter' in str(df_train['source']))\n",
        "df_train['source'] = df_train['source'].astype(int)\n",
        "# 3. emoji in bio as count & fill NaN\n",
        "import re\n",
        "df_train['emoji_in_bio'] = df_train['emoji_in_bio'].apply(lambda x: len(re.findall(r'[\\U0001f600-\\U0001f650]', str(x))))\n",
        "# 4. dates to timestamps\n",
        "import numpy as np\n",
        "df_train['created_at_x'] = pd.to_datetime(df_train['created_at_x']).astype(int)\n",
        "df_train['created_at_y'] = pd.to_datetime(df_train['created_at_y']).astype(int)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq2MoOD2hhRN"
      },
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "## We divide the training set into train and test making sure that there is no user overlap among sets, in order to avoid overfitting.\n",
        "train_inds, test_inds = next(GroupShuffleSplit(test_size=0.30, n_splits=2, random_state = 7).split(df_train, groups=df_train['user_id']))\n",
        "train = df_train.iloc[train_inds]\n",
        "test = df_train.iloc[test_inds]\n",
        "\n",
        "X_train = train.drop(['tweet_id', 'user_id', 'text', 'label'], 1) # we don't need user ID, tweet Id, text or truth label in this set.\n",
        "X_test = test.drop(['tweet_id', 'user_id', 'text', 'label'], 1) # we don't need user ID, tweet Id, text or truth label in this set.\n",
        "y_train = train['label'] # truth labels\n",
        "y_test = test['label']"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsNKSiGyEPdP"
      },
      "source": [
        "def CreateBalancedSampleWeights(y_train, largest_class_weight_coef):\n",
        "    classes = np.unique(y_train, axis = 0)\n",
        "    classes.sort()\n",
        "    class_samples = np.bincount(y_train)\n",
        "    total_samples = class_samples.sum()\n",
        "    n_classes = len(class_samples)\n",
        "    weights = total_samples / (n_classes * class_samples * 1.0)\n",
        "    class_weight_dict = {key : value for (key, value) in zip(classes, weights)}\n",
        "    class_weight_dict[classes[1]] = class_weight_dict[classes[1]] * largest_class_weight_coef\n",
        "    sample_weights = [class_weight_dict[y] for y in y_train]\n",
        "    return sample_weights"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz6jlYcX6wIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186c3b6f-d117-4384-d888-ec3a20b26504"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "largest_class_weight_coef = max(df_train['label'].value_counts().values)/df_train.shape[0]\n",
        "weight = CreateBalancedSampleWeights(y_train, largest_class_weight_coef)\n",
        "\n",
        "param_dist = {'objective':'multi:softmax', 'num_class': 3, 'eta': 0.3, 'max_depth':6, 'random_state': 24}\n",
        "xg = xgb.XGBClassifier(**param_dist,  weights = weight)\n",
        "bst = xg.fit(X_train, y_train)\n",
        "preds = bst.predict(X_test)\n",
        "print(metrics.classification_report(y_test, preds, digits=3))\n",
        "fval = f1_score(y_test, preds, average=None)\n",
        "print('F1 score average (Favour, against): ', (fval[0]+ fval[1])/2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.913     0.741     0.818       170\n",
            "           1      0.705     0.771     0.736       292\n",
            "           2      0.609     0.625     0.617       192\n",
            "\n",
            "    accuracy                          0.720       654\n",
            "   macro avg      0.743     0.712     0.724       654\n",
            "weighted avg      0.731     0.720     0.723       654\n",
            "\n",
            "F1 score average (Favour, against):  0.7773396815950007\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}